"""Backend supported: tensorflow.compat.v1, tensorflow, pytorch"""
import deepxde as dde
import numpy as np


def ddy(x, y):
    return dde.grad.hessian(y, x)
    """Вычислить матрицу Гессе H: H [i] [j] = d ^ 2y / dx_i dx_j, где i, j = 0, ..., dim_x-1.
    Используйте эту функцию для вычисления производных второго порядка вместо tf.gradients ()
    или torch.autograd.grad (), потому что
    - Это ленивая оценка, т. Е. Она вычисляет H [i] [j] только тогда, когда это необходимо.
    - Он запомнит градиенты, которые уже были вычислены, чтобы избежать дублирования
      вычисление.
    Аргументы:
        ys: выходной тензор формы (batch_size, dim_y).
        xs: входной тензор формы (batch_size, dim_x).
        component: Если dim_y> 1, то `ys [:, component]` используется как y для вычисления
            Гессен. Если dim_y = 1, «component» должен быть «None».
        я (число):
        j (число):
        grad_y: градиент y относительно `xs`. Если известно, что нужно избегать, укажите "grad_y".
            дублирование вычислений. «grad_y» может быть вычислен из «якобиана». Даже если
            вы не предоставляете `grad_y`, нет повторяющихся вычислений, если вы используете
            `` якобиан '' для вычисления производных первого порядка."""
  
def dddy(x, y):
    return dde.grad.jacobian(ddy(x, y), x)
"""Вычислить матрицу Якоби J: J [i] [j] = dy_i / dx_j, где i = 0, ..., dim_y - 1 и
    j = 0, ..., dim_x - 1.
    Используйте эту функцию для вычисления производных первого порядка вместо tf.gradients ().
    или torch.autograd.grad (), потому что
    - Это ленивая оценка, т. Е. Она вычисляет J [i] [j] только тогда, когда это необходимо.
    - Он запомнит градиенты, которые уже были вычислены, чтобы избежать дублирования
      вычисление.
    Аргументы:
        ys: выходной тензор формы (batch_size, dim_y).
        xs: входной тензор формы (batch_size, dim_x).
        я (число):
        j (int или None):
    Возврат:
        J [`i`] [` j`] в матрице Якоби J. Если `j` равно` None`, возвращает градиент
        y_i, то есть J [`i`]."""

def pde(x, y):
    dy_xx = ddy(x, y)
    dy_xxxx = dde.grad.hessian(dy_xx, x)
    return dy_xxxx + 1


def boundary_l(x, on_boundary):
    return on_boundary and np.isclose(x[0], 0)


def boundary_r(x, on_boundary):
    return on_boundary and np.isclose(x[0], 1)


def func(x):
    return -(x ** 4) / 24 + x ** 3 / 6 - x ** 2 / 4


geom = dde.geometry.Interval(0, 1)

bc1 = dde.DirichletBC(geom, lambda x: 0, boundary_l)
"""Граничные условия Дирихле: y (x) = func (x)"""
bc2 = dde.NeumannBC(geom, lambda x: 0, boundary_l)
"""Граничные условия Неймана: dy / dn (x) = func (x)."""
bc3 = dde.OperatorBC(geom, lambda x, y, _: ddy(x, y), boundary_r)
"""Общие граничные условия оператора: func (входы, выходы, X) = 0.
    Аргументы:
        geom: `` Геометрия ''.
        func: функция принимает аргументы (`входы`,` выходы`, `X`)
            и выводит тензор размера N x 1, где N - длина входов.
            «входы» и «выходы» - тензоры входа и выхода сети соответственно;
            `X` - это массив NumPy входов.
        on_boundary: (x, Geometry.on_boundary (x)) -> Истина / Ложь."""
bc4 = dde.OperatorBC(geom, lambda x, y, _: dddy(x, y), boundary_r)

data = dde.data.PDE(
    geom,
    pde,
    [bc1, bc2, bc3, bc4],
    num_domain=10,
    num_boundary=2,
    solution=func,
    num_test=100,
)
"""ODE или независимый от времени решатель PDE.
    Аргументы:
        геометрия: Экземпляр `` Геометрия ''.
        pde: глобальное PDE или список PDE. `` Нет '', если нет глобального PDE.
        bcs: граничное условие или список граничных условий. Используйте «[]«, если нет
            граничное условие.
        num_domain (int): количество обучающих точек, выбранных внутри домена.
        num_boundary (int): количество обучающих точек, выбранных на границе.
        train_distribution (строка): распределение по выборочным точкам обучения. Один из
            следующие: «равномерное» (сетка с равными интервалами), «псевдо» (псевдослучайное), «LHS»
            (Латинская выборка гиперкуба), «Халтон» (последовательность Халтона), «Хаммерсли»
            (Последовательность Хаммерсли) или "Соболь" (последовательность Соболя).
        якоря: массив Numpy точек обучения в дополнение к `num_domain` и
            Выборочные точки num_boundary.
        исключения: массив Numpy точек, которые нужно исключить для обучения.
        решение: эталонное решение.
        num_test: количество точек, отобранных внутри домена для тестирования. Тестирование
            точки на границе - это тот же набор точек, который использовался для обучения. Если
            `` Нет '', то тренировочные очки будут использованы для тестирования.
        a additional_var_function: функция, которая вводит `train_x` или` test_x` и выводит
            вспомогательные переменные."""
layer_size = [1] + [20] * 3 + [1]
#нейросеть состоит из 1 входного, 3 скрытых слоёв по 20 логических "нейронов" и 1 выходного слоя
activation = "tanh"
функция активации тангенс
initializer = "Glorot uniform"
"""Инициализатор формы Glorot, также называемый инициализатором формы Xavier.
Рисует образцы из равномерного распределения внутри [-limit, limit],
 где limit = sqrt(6 / (fan_in + fan_out))( fan_in- количество входных единиц в тензоре весов, 
 а fan_out- количество выходных единиц)."""
net = dde.maps.FNN(layer_size, activation, initializer)
#Полносвязная нейронная сеть
model = dde.Model(data, net)
model.compile("adam", lr=0.001, metrics=["l2 relative error"])
#Adam [1] - это адаптивный алгоритм оптимизации скорости обучения, разработанный специально для обучения глубоких нейронных сетей
losshistory, train_state = model.train(epochs=10000)
#Обучает модель для фиксированного количества эпох (итераций в наборе данных)
dde.saveplot(losshistory, train_state, issave=True, isplot=True)
#Сохраните / нанесите на график лучший тренированный результат и историю потерь.
#    Эта функция используется для быстрой проверки ваших результатов.
